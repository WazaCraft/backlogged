{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WazaCraft/backlogged/blob/main/REL_deal_id_api_01078.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deal-Identification-ID-API 0.1.7.7 (hotfix)\n",
        "##LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "\n",
        "\n",
        "> Rel: July 2, 2023 Version: 0.1.7\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KEeSqBFK9q66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "# Author: Johnathan Greenaway\n",
        "# Organization: StackCommerce Inc.\n",
        "# Release Date: July 2, 2023\n",
        "#0.1.7.7\n",
        "#Fixed Flask response route and reference\n",
        "#Serious error revisions\n",
        "#0.1.7.5\n",
        "#Fixed embedding naming\n",
        "#0.1.7.4\n",
        "#Reintroduced Pickle\n",
        "#0.1.7.3\n",
        "#Deal-Identification-ID-API 0.1.7\n",
        "#LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "#Added commands to enable / disable Flask server for REST API\n",
        "#Added reconfig server port path\n",
        "#0.1.7.2\n",
        "#Added Flask\n",
        "\n",
        "#To do: add vector selection\n",
        "!pip install openai\n",
        "!pip install bs4\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install scikit-learn\n",
        "!pip install Flask"
      ],
      "metadata": {
        "id": "qdSUyifVPsqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import openai\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pickle\n",
        "import socket\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "openai_api_key = input(\"Please enter your OpenAI API key: \")\n",
        "openai.api_key = openai_api_key\n",
        "os.environ['USER_PROMPT'] = 'Here is the info from the text: {content}. Based on this, what is the answer to \"{question}\"?'\n",
        "\n",
        "def chunk_text(text, max_tokens=8000):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 > max_tokens:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        current_chunk.append(word)\n",
        "        current_length += len(word) + 1\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def get_embedding_for_large_text(text):\n",
        "    chunks = chunk_text(text)\n",
        "    embeddings = []\n",
        "    for chunk in chunks:\n",
        "        response = openai.Embedding.create(input=chunk, model=\"text-embedding-ada-002\")\n",
        "        embedding = response['data'][0]['embedding']\n",
        "        embeddings.append(embedding)\n",
        "    return embeddings\n",
        "\n",
        "def create_file_name(url, extension='txt'):\n",
        "    parsed_url = urlparse(url)\n",
        "    url_path_parts = parsed_url.path.strip('/').split('/')\n",
        "    last_part = url_path_parts[-1] if url_path_parts else parsed_url.netloc\n",
        "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    return f\"{last_part}-{current_date}.{extension}\"\n",
        "\n",
        "def get_most_similar_text_chunk(question, embeddings_dict):\n",
        "    question_embedding = get_embedding_for_large_text(question)[0]\n",
        "    similarity_scores = []\n",
        "    for text_chunk_embedding in embeddings_dict['embeddings']:\n",
        "        similarity_scores.append(cosine_similarity([question_embedding], [text_chunk_embedding])[0][0])\n",
        "    most_similar_index = np.argmax(similarity_scores)\n",
        "    return embeddings_dict['text_chunks'][most_similar_index]\n",
        "\n",
        "def generate_response(question, embeddings_dict):\n",
        "    similar_text_chunk = get_most_similar_text_chunk(question, embeddings_dict)\n",
        "    user_prompt = os.environ['USER_PROMPT'].format(content=similar_text_chunk, question=question)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(model=\"gpt-4\", messages=messages)\n",
        "        assistant_reply = response['choices'][0]['message']['content']\n",
        "        return assistant_reply\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "def extract_and_save_urls(html_content, file):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    for link in soup.find_all('a'):\n",
        "        url = link.get('href')\n",
        "        if url:\n",
        "            file.write(url + '\\n')\n",
        "\n",
        "def save_embeddings_to_file(embeddings_dict, file_name):\n",
        "    with open(file_name, 'wb') as file:\n",
        "        pickle.dump(embeddings_dict, file)\n",
        "\n",
        "def load_embeddings_from_file(file_name):\n",
        "    with open(file_name, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "embeddings_dict = {}\n",
        "\n",
        "url = 'https://www.rssground.com/services/rss-converter/64a0a74cd5ee7/RSS-Payload'\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "file_name = create_file_name(url)\n",
        "\n",
        "with open(file_name, 'w') as file:\n",
        "    file.write(text)\n",
        "    extract_and_save_urls(text, file)\n",
        "\n",
        "embeddings = get_embedding_for_large_text(text)\n",
        "chunks = chunk_text(text)\n",
        "embeddings_file_name = create_file_name(url, extension='pkl')\n",
        "embeddings_dict[embeddings_file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "save_embeddings_to_file(embeddings_dict, embeddings_file_name)\n",
        "\n",
        "print(\"Daily data refreshed. Now browsing 75+ deal feeds.\")\n",
        "\n",
        "@app.route('/ask', methods=['GET'])\n",
        "def ask_question():\n",
        "    question = request.args.get('question')\n",
        "    if question:\n",
        "        responses = []\n",
        "        for embeddings_file_name in embeddings_dict.keys():\n",
        "            response = generate_response(question, embeddings_dict[embeddings_file_name])\n",
        "            responses.append(response)\n",
        "        return jsonify(responses)\n",
        "    return jsonify({\"error\": \"No question provided\"})\n",
        "\n",
        "def run_web_api(port):\n",
        "    app.run(port=port)\n",
        "\n",
        "def is_port_in_use(port):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        return s.connect_ex(('localhost', port)) == 0\n",
        "\n",
        "api_thread = None\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter URL or question or 'deal-id up' or 'deal-id down' (or 'exit' to quit): \")\n",
        "\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    elif user_input.lower() == 'deal-id up':\n",
        "        if api_thread is None or not api_thread.is_alive():\n",
        "            port = 5000\n",
        "            while is_port_in_use(port):\n",
        "                port = int(input(f\"Port {port} is in use. Please enter a different port: \"))\n",
        "            api_thread = threading.Thread(target=run_web_api, args=(port,))\n",
        "            api_thread.daemon = True\n",
        "            api_thread.start()\n",
        "        else:\n",
        "            print(\"Server is already running\")\n",
        "\n",
        "    elif user_input.lower() == 'deal-id down':\n",
        "        if api_thread and api_thread.is_alive():\n",
        "            print(\"Stopping the server.\")\n",
        "            requests.post(f'http://localhost:{port}/shutdown')\n",
        "            api_thread.join()\n",
        "        else:\n",
        "            print(\"Server is not running\")\n",
        "\n",
        "    elif user_input.lower().startswith('http'):\n",
        "        url = user_input\n",
        "        response = requests.get(url)\n",
        "        text = response.text\n",
        "        file_name = create_file_name(url)\n",
        "\n",
        "        with open(file_name, 'w') as file:\n",
        "            file.write(text)\n",
        "            extract_and_save_urls(text, file)\n",
        "\n",
        "        embeddings = get_embedding_for_large_text(text)\n",
        "        chunks = chunk_text(text)\n",
        "        embeddings_file_name = create_file_name(url, extension='pkl')\n",
        "        embeddings_dict[embeddings_file_name] = {'text_chunks': chunks, 'embeddings': embeddings}\n",
        "        save_embeddings_to_file(embeddings_dict, embeddings_file_name)\n",
        "\n",
        "    else:\n",
        "        question = user_input\n",
        "        for embeddings_file_name in embeddings_dict.keys():\n",
        "            response = generate_response(question, embeddings_dict[embeddings_file_name])\n",
        "            print(response)\n"
      ],
      "metadata": {
        "id": "B1M9XS7APnOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deal ID API Documentation\n",
        "\n",
        "### LLM Knowledge Assistant with Customizable Prompt Wrapper for Deal Identification\n",
        "### Author: Johnathan Greenaway\n",
        "#### Organization: StackCommerce Inc.\n",
        ">Release Date: July 2, 2023\n",
        "> Version: 0.1.7\n",
        "##Overview\n",
        "This API allows you to interact with the Deal ID service which provides information based on a dataset of 75+ deal feeds. The data is refreshed daily.\n",
        "\n",
        "##Endpoint\n",
        "GET /ask\n",
        "\n",
        "###Description\n",
        "Retrieves answers to questions based on the information from the text dataset.\n",
        "\n",
        "###Parameters\n",
        "####Name\tType\tIn\tDescription\n",
        "question\tstring\tquery\tThe question you want to ask based on the text dataset.\n",
        "Responses\n",
        "200 OK\n",
        "Successful response.\n",
        "\n",
        "###Content:\n",
        "\n",
        "A JSON array containing the answers to the question based on the data available in the text dataset.\n",
        "400 Bad Request\n",
        "Occurs when no question parameter is provided in the request.\n",
        "\n",
        "###Content:\n",
        "\n",
        "A JSON object containing an error message.\n",
        "Example\n",
        "Request\n",
        "\n",
        "http\n",
        "Copy code\n",
        "GET http://localhost:5000/ask?question=What%20are%20the%20latest%20deals%20available?\n",
        "Response\n",
        "\n",
        "json\n",
        "Copy code\n",
        "[\n",
        "    \"The latest deals available are ...\",\n",
        "    \"Another source suggests that ...\"\n",
        "]\n",
        "Usage Instructions\n",
        "Python Script\n",
        "Use the requests library in Python:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import requests\n",
        "\n",
        "# Define the endpoint URL\n",
        "url = \"http://localhost:5000/ask\"\n",
        "\n",
        "# Define the question you want to ask\n",
        "question = \"What are the latest deals available?\"\n",
        "\n",
        "# Make the GET request\n",
        "response = requests.get(url, params={\"question\": question})\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Print the answer\n",
        "    print(response.json())\n",
        "else:\n",
        "    # Print error message\n",
        "    print(\"Error:\", response.json())\n",
        "Command Line (using curl)\n",
        "Use curl command in the terminal or command prompt:\n",
        "\n",
        "sh\n",
        "Copy code\n",
        "curl \"http://localhost:5000/ask?question=What%20are%20the%20latest%20deals%20available?\"\n",
        "Web Browser\n",
        "Enter the URL directly in the address bar of your web browser:\n",
        "\n",
        "perl\n",
        "Copy code\n",
        "http://localhost:5000/ask?question=What%20are%20the%20latest%20deals%20available?\n",
        "Please make sure the server is running on the correct port before making the request. You can start the server by running the script and entering deal-id up in the console. Additionally, if the port 5000 is already in use, you may need to specify a different port number.\n",
        "\n",
        "\n",
        "##Description:\n",
        "#### This script creates a Knowledge Assistant that interacts with the user through the console.\n",
        "####The user can input a URL, and the assistant will fetch the text content from that URL, extract embeddings for similarity matching, and store it for future querying.\n",
        "#The user can also input questions, and the assistant will find the most similar text chunk from the stored content and generate responses using OpenAI's GPT-4 API.\n",
        "\n",
        "Libraries Used:\n",
        "- openai: For interacting with OpenAI's GPT-4 API.\n",
        "- bs4 (BeautifulSoup): For parsing HTML and extracting text from web pages.\n",
        "- requests: For making HTTP requests to fetch web pages.\n",
        "- scikit-learn: For calculating cosine similarity between embeddings.\n",
        "- numpy: For numerical operations such as finding argmax.\n",
        "\n",
        "Features:\n",
        "- Set Environmental Variables: `OPENAI_API_KEY` and `USER_PROMPT`.\n",
        "- Function to split text into smaller chunks.\n",
        "- Function to get embeddings for large texts.\n",
        "- Function to parse the URL and create a file name.\n",
        "- Function to get the most similar text chunk.\n",
        "- Function to generate a response based on the question and embeddings.\n",
        "- Function to extract and save URLs from HTML content.\n",
        "- Infinite loop for user interaction.\n",
        "\n",
        "# Changelog for Version 0.1.5:\n",
        "##0.1.0:\n",
        "- Initial version.\n",
        "## 0.1.2:\n",
        "- Added environmental variable for prompt customization.\n",
        "- Added function to get embeddings for large texts.\n",
        "- Added function to split text into smaller chunks.\n",
        "- Added function to parse the URL and create a file name.\n",
        "- Added function to find the most similar text chunk.\n",
        "- Added function to generate responses based on questions and embeddings.\n",
        "- Stored text chunks and embeddings in a dictionary.\n",
        "- Added functionality for user interaction in an infinite loop.\n",
        "# 0.1.3:\n",
        "- Set a default URL to be loaded on startup.\n",
        "- Added message \"Daily data refreshed. Now browsing 75+ deal feeds.\".\n",
        "# 0.1.4:\n",
        "- Extracted all URLs from the provided link.\n",
        "- Stored extracted URLs in the same plain text file.\n",
        "#0.1.5:\n",
        "- Removed default OpenAI API key.\n",
        "- Added user prompt to enter their OpenAI API key.\n",
        "\n",
        "# Libraries Used:\n",
        "- openai: For interacting with OpenAI's GPT-4 API.\n",
        "- bs4 (BeautifulSoup): For parsing HTML and extracting text from web pages.\n",
        "- requests: For making HTTP requests to fetch web pages.\n",
        "- scikit-learn: For calculating cosine similarity between embeddings.\n",
        "- numpy: For numerical operations such as finding argmax."
      ],
      "metadata": {
        "id": "l-_om37Tzlih"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o079L0AsAr9k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}